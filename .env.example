# ------------------------------------------------------------------------------
# Next.js React Notion X Environment Variables
#
# This file contains all the environment variables needed to run the application.
# Copy this file to .env.local and fill in the values.
#
# - Required variables must be set for the application to run.
# - Optional variables can be left blank if you don't need the feature.
# ------------------------------------------------------------------------------

# -- Required ------------------------------------------------------------------

# Notion
# Find your Notion root page ID by opening the page in your browser and
# copying the ID from the URL. It's the 32-character string at the end.
# e.g. https://www.notion.so/your-workspace/My-Page-0123456789abcdef0123456789abcdef
NOTION_ROOT_PAGE_ID=

# -- AI / RAG / Chat Features --------------------------------------------------
# These are required for the AI chat functionality.

# OpenAI API Key
# Get yours from https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Google Gemini (optional)
# Obtain a key from https://aistudio.google.com/app/apikey
# GOOGLE_API_KEY=

# Model selection (provider + model combined)
# DEFAULT_LLM_MODEL="OpenAI gpt-4o-mini"
# EMBEDDING_MODEL="OpenAI text-embedding-3-small (v1)"
# EMBEDDING_SPACE_ID=openai_te3s_v1  # optional, inferred from EMBEDDING_MODEL when omitted

# Supabase credentials for vector storage (RAG)
# Get these from your Supabase project settings.
SUPABASE_URL=https://<your-supabase-project>.supabase.co
SUPABASE_ANON_KEY=
SUPABASE_SERVICE_ROLE_KEY=

# -- Optional ------------------------------------------------------------------

# Admin Dashboard Basic Auth
# Protects the /admin routes with a username and password.
ADMIN_DASH_USER=
ADMIN_DASH_PASS=

# AI / RAG / Chat Configuration (has defaults)
# You can override the default models and parameters for the AI chat.
# DEFAULT_ENGINE=lc    # or 'native'. 'lc' for LangChain, 'native' for direct OpenAI API.
# DEFAULT_LLM_MODEL="OpenAI gpt-4o-mini"
# EMBEDDING_MODEL="OpenAI text-embedding-3-small (v1)"
# EMBEDDING_SPACE_ID=openai_te3s_v1
# LLM_TEMPERATURE=0.0
# RAG_TOP_K=5
# RAG_SIMILARITY_THRESHOLD=0.78
# CHAT_CONTEXT_TOKEN_BUDGET=1200
# CHAT_CONTEXT_CLIP_TOKENS=320
# CHAT_HISTORY_TOKEN_BUDGET=900
# CHAT_SUMMARY_ENABLED=true
# CHAT_SUMMARY_TRIGGER_TOKENS=400
# CHAT_SUMMARY_MAX_TURNS=6
# CHAT_SUMMARY_MAX_CHARS=600
# CHAT_CHITCHAT_KEYWORDS="hello,hi,how are you,whats up,what is up,tell me a joke"
# CHAT_FALLBACK_CHITCHAT_CONTEXT="This is a light-weight chit-chat turn. Keep the response concise, warm, and avoid citing the knowledge base."
# CHAT_FALLBACK_COMMAND_CONTEXT="The user is asking for an action/command. You must politely decline to execute actions and instead explain what is possible."
# RAG metadata defaults; leave blank if you don't want a fallback doc/persona type
RAG_DEFAULT_DOC_TYPE=kb_article
RAG_DEFAULT_PERSONA_TYPE=professional

# Langfuse tracing (optional but recommended for observability)
# LANGFUSE_PUBLIC_KEY=
# LANGFUSE_SECRET_KEY=
# LANGFUSE_HOST=https://cloud.langfuse.com
# LANGFUSE_ENV_TAG=dev
# LANGFUSE_SAMPLE_RATE_DEV=0.3
# LANGFUSE_SAMPLE_RATE_PREVIEW=1
# LANGFUSE_ATTACH_PROVIDER_METADATA=true

# ---------- Global log levels ----------
# LOG_GLOBAL_LEVEL=debug          # off | error | info | debug | trace
# LOG_RAG_LEVEL=debug
# LOG_INGESTION_LEVEL=debug
# LOG_NOTION_LEVEL=info
# LOG_LLM_LEVEL=info
# LOG_DB_LEVEL=debug             # off | error | info | debug | trace (db operations)

# ---------- Telemetry / Langfuse ----------
# TELEMETRY_ENABLED=true
# TELEMETRY_SAMPLE_RATE_DEFAULT=1
# TELEMETRY_SAMPLE_RATE_MAX=1
# TELEMETRY_DETAIL_DEFAULT=standard
# TELEMETRY_DETAIL_MAX=verbose     # lower (e.g. "standard") for production

# Non-production overrides (debugging only):
# TELEMETRY_DETAIL_OVERRIDE=verbose
# TELEMETRY_SAMPLE_RATE_OVERRIDE=1

# Local LLM backend configuration (set LOCAL_LLM_BACKEND to "ollama" or "lmstudio")
# LOCAL_LLM_BACKEND=ollama
# OLLAMA_BASE_URL=http://127.0.0.1:11434
# LMSTUDIO_BASE_URL=http://127.0.0.1:1234/v1
# LMSTUDIO_API_KEY=
# OLLAMA_MODEL_DEFAULT=mistral
# OLLAMA_ENABLE_IN_PROD=false
# OLLAMA_TIMEOUT_MS=30000
# OLLAMA_MAX_TOKENS=1024

# Analytics
# NEXT_PUBLIC_FATHOM_ID=
# NEXT_PUBLIC_POSTHOG_ID=
# NEXT_PUBLIC_NOTION_POLISH_PROFILE=balanced # balanced | legacy

# Twitter/X Integration
# For rendering tweets more efficiently.
# TWITTER_ACCESS_TOKEN=

# Redis for Preview Image Caching
# See site.config.ts to enable (isRedisEnabled).
# REDIS_HOST=
# REDIS_PASSWORD=
# REDIS_USER='default'
# REDIS_NAMESPACE='preview-images'

# Notion API Overrides (for advanced use cases like self-hosted Notion)
# NOTION_API_BASE_URL=
# NOTION_PAGE_CACHE_TTL=60 # Overrides default cache TTL in seconds.
